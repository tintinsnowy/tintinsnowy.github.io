<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tech Tool on Sherry&#39;s blog</title>
    <link>http://tintinsnowy.com/tags/tech-tool/</link>
    <description>Recent content in Tech Tool on Sherry&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 14 Mar 2016 09:06:34 +0800</lastBuildDate>
    <atom:link href="http://tintinsnowy.com/tags/tech-tool/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Spark-多节点集群配置</title>
      <link>http://tintinsnowy.com/2016/03/14/spark-%E5%A4%9A%E8%8A%82%E7%82%B9%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE</link>
      <pubDate>Mon, 14 Mar 2016 09:06:34 +0800</pubDate>
      
      <guid>http://tintinsnowy.com/2016/03/14/spark-%E5%A4%9A%E8%8A%82%E7%82%B9%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE</guid>
      <description>

&lt;h1 id=&#34;楔子:d7f0df7a88d75431aefaa0c11af54130&#34;&gt;楔子&lt;/h1&gt;

&lt;p&gt;这次完全拿到的是裸机，所以从零开始配置。其实集群和单节点差不多，见我前面的blog&lt;/p&gt;

&lt;h2 id=&#34;本机配置:d7f0df7a88d75431aefaa0c11af54130&#34;&gt;本机配置&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;Centos 5.8&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;4 cores 8G&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&#34;节点布置-masters-slaves:d7f0df7a88d75431aefaa0c11af54130&#34;&gt;节点布置 Masters&amp;amp;Slaves&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Master   119.254.168.33&lt;/li&gt;
&lt;li&gt;Slaves1  119.254.168.34&lt;/li&gt;
&lt;li&gt;Slaves2  119.254.168.36&lt;/li&gt;
&lt;li&gt;Slaves3  119.254.168.38&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;环境配置-environment:d7f0df7a88d75431aefaa0c11af54130&#34;&gt;环境配置 Environment&lt;/h1&gt;

&lt;h3 id=&#34;java-环境:d7f0df7a88d75431aefaa0c11af54130&#34;&gt;JAVA 环境&lt;/h3&gt;

&lt;p&gt;见&lt;code&gt;Apache Spark单节点安装和环境配置&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&#34;scala-环境:d7f0df7a88d75431aefaa0c11af54130&#34;&gt;SCALA 环境&lt;/h3&gt;

&lt;p&gt;见&lt;code&gt;Apache Spark单节点安装和环境配置&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&#34;ssh-配置:d7f0df7a88d75431aefaa0c11af54130&#34;&gt;SSH 配置&lt;/h3&gt;

&lt;p&gt;背景：搭建Hadoop环境需要设置无密码登陆，所谓无密码登陆其实是指通过证书认证的方式登陆 ，使用一种被称为&amp;rdquo;公私钥&amp;rdquo;(RSA)认证的方式来进行ssh登录。
在linux系统中,ssh是远程登录的默认工具,因为该工具的协议使用了RSA/DSA的加密算法.该工具做linux系统的远程管理是非常安全的。&lt;/p&gt;

&lt;p&gt;所谓ssh就是ssh免密码登录服务器，其中用到了RSA加密算法。其中的细节和原理我有时间再写。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;确保安装好&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ssh：（ubuntu版）
$ sudo apt-get update
$ sudo apt-get install openssh-server
$ sudo /etc/init.d/ssh start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ssh(centos): 确认系统已经安装了SSH。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rpm –qa | grep openssh
rpm –qa | grep rsync


yum install ssh //安装SSH协议
yum install rsync //rsync是一个远程数据同步工具，可通过LAN/WAN快速同步多台主机间的文件
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;service sshd restart &amp;ndash;&amp;gt;启动服务
2. 生成并添加密钥：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    $ ssh-keygen -t rsa
    $ cat ~/.ssh/id_rsa.pub &amp;gt;&amp;gt; ~/.ssh/authorized_keys
    $ chmod 0600 ~/.ssh/authorized_keys
    service sshd restart //一般修改过都需要重启服务
如果已经生成过密钥，只需执行后两行命令。
测试ssh localhost

    $ ssh localhost
    $ exit

查看端口：是否打开

    netstat -anp |grep ssh
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;hadoop-cluster-installation:d7f0df7a88d75431aefaa0c11af54130&#34;&gt;Hadoop cluster Installation&lt;/h1&gt;

&lt;p&gt;基本和前面相同&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;修改hdfs-site.xml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;configuration&amp;gt;
    &amp;lt;property&amp;gt;
         &amp;lt;name&amp;gt;dfs.namenode.secondary.http-address&amp;lt;/name&amp;gt;
         &amp;lt;value&amp;gt;kexinyun1:9001&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;


    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;dfs.namenode.name.dir&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;file:///opt/hadoop-2.6.1/dfs/name&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;


    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;dfs.datanode.data.dir&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;file:///opt/hadoop-2.6.1/dfs/data&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;


    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;dfs.replication&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;3&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;


    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;dfs.webhdfs.enabled&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/configuration&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;mapred-site.xml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;configuration&amp;gt;  
    &amp;lt;property&amp;gt;  
&amp;lt;name&amp;gt;mapreduce.framework.name&amp;lt;/name&amp;gt;  
&amp;lt;value&amp;gt;yarn&amp;lt;/value&amp;gt;  
    &amp;lt;/property&amp;gt;  
&amp;lt;property&amp;gt;  
    &amp;lt;name&amp;gt;mapreduce.jobtracker.http.address&amp;lt;/name&amp;gt;  
    &amp;lt;value&amp;gt;nameNode:50030&amp;lt;/value&amp;gt;  
&amp;lt;/property&amp;gt;  
&amp;lt;property&amp;gt;  
    &amp;lt;name&amp;gt;mapreduce.jobhistory.address&amp;lt;/name&amp;gt;  
    &amp;lt;value&amp;gt;nameNode:10020&amp;lt;/value&amp;gt;  
&amp;lt;/property&amp;gt;  
&amp;lt;property&amp;gt;  
    &amp;lt;name&amp;gt;mapreduce.jobhistory.webapp.address&amp;lt;/name&amp;gt;  
    &amp;lt;value&amp;gt;nameNode:19888&amp;lt;/value&amp;gt;  
&amp;lt;/property&amp;gt;  
&amp;lt;/configuration&amp;gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;yarn 修改&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;configuration&amp;gt;
    &amp;lt;!-- Site specific YARN configuration properties --&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;yarn.nodemanager.aux-services&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;mapreduce_shuffle&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;yarn.resourcemanager.address&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;kexinyun1:8032&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;yarn.resourcemanager.scheduler.address&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;kexinyun1:8030&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;yarn.resourcemanager.resource-tracker.address&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;kexinyun1:8031&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;yarn.resourcemanager.admin.address&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;kexinyun1:8033&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;yarn.resourcemanager.webapp.address&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;kexinyun1:8088&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;slaves 文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;119.254.168.38 //(slaves3)
119.254.168.36 //(slaves2)
119.254.168.34 //(slaves1)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;vi hadoop-env.sh&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export JAVA_HOME=your java home
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;vi yarn-env.sh&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    export JAVA_HOME=your java home   
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;格式化（同以前）&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;启动/停止&lt;/p&gt;

&lt;p&gt;查看：jsp&lt;/p&gt;

&lt;p&gt;访问:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://ip:9001/&#34;&gt;http://ip:9001/&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;spark-cluster-installation:d7f0df7a88d75431aefaa0c11af54130&#34;&gt;Spark Cluster Installation&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;基本同单节点类似&lt;/li&gt;

&lt;li&gt;&lt;p&gt;文件配置部分&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export SCALA_HOME=/opt/scala-2.11.4
export JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.101.x86_64/
export SPARK_HOME=/opt/spark
export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop
export SPARK_JAR=/opt/spark/lib/spark-assembly-1.6.1-hadoop2.6.0.jar
export SPARK_MASTER_IP=localhost
export SPARK_MASTER_PORT=7077
export SPARK_WORKER_CORES=1
export SPARK_WORKER_INSTANCES=1
export SPARK_WORKER_MEMORY=1g
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;启动&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$SPARK_HOME/sbin/start-all.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;problem:d7f0df7a88d75431aefaa0c11af54130&#34;&gt;problem&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;http://www.2cto.com/os/201209/155681.html&#34;&gt;http://www.2cto.com/os/201209/155681.html&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;english-version:d7f0df7a88d75431aefaa0c11af54130&#34;&gt;english version&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;http://pingax.com/install-hadoop2-6-0-on-ubuntu/&#34;&gt;http://pingax.com/install-hadoop2-6-0-on-ubuntu/&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;reference:d7f0df7a88d75431aefaa0c11af54130&#34;&gt;Reference&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cnblogs.com/lanxuezaipiao/p/3525554.html&#34;&gt;http://www.cnblogs.com/lanxuezaipiao/p/3525554.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://pingax.com/install-hadoop2-6-0-on-ubuntu/&#34;&gt;http://pingax.com/install-hadoop2-6-0-on-ubuntu/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/ClusterSetup.html&#34;&gt;http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/ClusterSetup.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://blog.csdn.net/greensurfer/article/details/39450369&#34;&gt;http://blog.csdn.net/greensurfer/article/details/39450369&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Ubuntu 下好用的pdf预览器-okular</title>
      <link>http://tintinsnowy.com/2016/01/12/ubuntu-%E4%B8%8B%E5%A5%BD%E7%94%A8%E7%9A%84pdf%E9%A2%84%E8%A7%88%E5%99%A8-okular</link>
      <pubDate>Tue, 12 Jan 2016 22:17:20 +0800</pubDate>
      
      <guid>http://tintinsnowy.com/2016/01/12/ubuntu-%E4%B8%8B%E5%A5%BD%E7%94%A8%E7%9A%84pdf%E9%A2%84%E8%A7%88%E5%99%A8-okular</guid>
      <description>

&lt;h1 id=&#34;楔子:e51f4326ef99271429324eb5b74cb903&#34;&gt;楔子&lt;/h1&gt;

&lt;h2 id=&#34;the-desiderata-part-4:e51f4326ef99271429324eb5b74cb903&#34;&gt;The Desiderata (part 4)&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;by Max Ehrmann, 1927&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Nurture strength of spirit to shield you in sudden misfortune.&lt;/p&gt;

&lt;p&gt;But do not distress yourself with dark imaginings.&lt;/p&gt;

&lt;p&gt;Many fears are born of fatigue and loneliness.&lt;/p&gt;

&lt;p&gt;Beyond a wholesome discipline,&lt;/p&gt;

&lt;p&gt;be gentle with yourself.&lt;/p&gt;

&lt;p&gt;You are a child of the universe,&lt;/p&gt;

&lt;p&gt;no less than the trees and the stars;&lt;/p&gt;

&lt;p&gt;you have a right to be here.&lt;/p&gt;

&lt;p&gt;And whether or not it is clear to you,&lt;/p&gt;

&lt;p&gt;no doubt the universe is unfolding as it should.&lt;/p&gt;

&lt;p&gt;-TBC&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;是的，这okular比Adobe PDF自己的预览器还好用，&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://okular.kde.org/&#34;&gt;官方网站&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;     sudo apt-get install okular
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;完成！有了好工具才能好工作～酱O(∩_∩)O&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>翻墙神器 lantern</title>
      <link>http://tintinsnowy.com/2016/01/08/%E7%BF%BB%E5%A2%99%E7%A5%9E%E5%99%A8-lantern</link>
      <pubDate>Fri, 08 Jan 2016 11:23:55 +0800</pubDate>
      
      <guid>http://tintinsnowy.com/2016/01/08/%E7%BF%BB%E5%A2%99%E7%A5%9E%E5%99%A8-lantern</guid>
      <description>

&lt;h1 id=&#34;楔子:653b18e85980fea4c791a575ea548276&#34;&gt;楔子&lt;/h1&gt;

&lt;h2 id=&#34;the-desiderata-part-3:653b18e85980fea4c791a575ea548276&#34;&gt;The Desiderata (part 3)&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;by Max Ehrmann, 1927&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Exercise caution in your business affairs,&lt;/p&gt;

&lt;p&gt;for the world is full of trickery.&lt;/p&gt;

&lt;p&gt;But let not this blind you to what virtue there is;&lt;/p&gt;

&lt;p&gt;many persons strive for high ideals,&lt;/p&gt;

&lt;p&gt;and everywhere life is full of heroism.&lt;/p&gt;

&lt;p&gt;Be yourself. Especially do not feign affection.&lt;/p&gt;

&lt;p&gt;Neither be cynical about love;&lt;/p&gt;

&lt;p&gt;for in the face of all aridity and&lt;/p&gt;

&lt;p&gt;disenchantment it is as perennial as the grass.&lt;/p&gt;

&lt;p&gt;Take kindly the counsel of the years,&lt;/p&gt;

&lt;p&gt;gracefully surrendering the things of youth.&lt;/p&gt;

&lt;p&gt;&amp;ndash;TBC&lt;/p&gt;

&lt;h1 id=&#34;资源:653b18e85980fea4c791a575ea548276&#34;&gt;资源&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Windows版       &lt;a href=&#34;http://pan.baidu.com/s/1hrgqgGc&#34;&gt;2016-1-5更新&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Mac版           &lt;a href=&#34;http://pan.baidu.com/s/1bpVRQY&#34;&gt;2016-1-5更新&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Android版       &lt;a href=&#34;http://pan.baidu.com/s/1i4vhrTzs&#34;&gt;2016-1-5更新&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Linux版         &lt;a href=&#34;http://pan.baidu.com/s/1eRnLLJG&#34;&gt;2016-1-5更新&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;ubuntu:653b18e85980fea4c791a575ea548276&#34;&gt;Ubuntu&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt;dpkg -i lantern-installer-beta-64-bit.deb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;安装完毕后&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;lantern 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;不要关闭terminal， 在浏览器可以打开了&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.iyaxi.com/wp-content/uploads/2015/11/20151117231654.jpg&#34; alt=&#34;lokk&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;p-s-deb文件的安装方法:653b18e85980fea4c791a575ea548276&#34;&gt;p.s.deb文件的安装方法&lt;/h1&gt;

&lt;p&gt;以下是一些 Dpkg 的普通用法：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;dpkg -i &lt;package.deb&gt;
安装一个 Debian 软件包，如你手动下载的文件。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;dpkg -c &lt;package.deb&gt;
列出 &lt;package.deb&gt; 的内容。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;dpkg -I &lt;package.deb&gt;
从 &lt;package.deb&gt; 中提取包裹信息。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;dpkg -r &lt;package&gt;
移除一个已安装的包裹。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;dpkg -P &lt;package&gt;
完全清除一个已安装的包裹。和 remove 不同的是，remove 只是删掉数据和可执行文件，purge 另外还删除所有的配制文件。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;dpkg -L &lt;package&gt;
列出 &lt;package&gt; 安装的所有文件清单。同时请看 dpkg -c 来检查一个 .deb 文件的内容。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;dpkg -s &lt;package&gt;
显示已安装包裹的信息。同时请看 apt-cache 显示 Debian 存档中的包裹信息，以及 dpkg -I 来显示从一个 .deb 文件中提取的包裹信息。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;dpkg-reconfigure &lt;package&gt;
重新配制一个已经安装的包裹，如果它使用的是 debconf (debconf 为包裹安装提供了一个统一的配制界面)。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;reference:653b18e85980fea4c791a575ea548276&#34;&gt;Reference&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://www.iyaxi.com/2015-11-17/732.html&#34;&gt;http://www.iyaxi.com/2015-11-17/732.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://blog.csdn.net/kevinhg/article/details/5934462&#34;&gt;ubuntu 中安装deb&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Apache Spark[单节点]安装和环境配置</title>
      <link>http://tintinsnowy.com/2016/01/08/apache-spark%E5%8D%95%E8%8A%82%E7%82%B9%E5%AE%89%E8%A3%85%E5%92%8C%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE</link>
      <pubDate>Fri, 08 Jan 2016 10:36:16 +0800</pubDate>
      
      <guid>http://tintinsnowy.com/2016/01/08/apache-spark%E5%8D%95%E8%8A%82%E7%82%B9%E5%AE%89%E8%A3%85%E5%92%8C%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE</guid>
      <description>

&lt;h1 id=&#34;楔子:1784e58cad9564fb26f40c083f1bce7b&#34;&gt;楔子&lt;/h1&gt;

&lt;h2 id=&#34;the-desiderata-part-2:1784e58cad9564fb26f40c083f1bce7b&#34;&gt;The Desiderata (part 2)&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;by Max Ehrmann, 1927&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;If you compare yourself with others,&lt;/p&gt;

&lt;p&gt;you may become vain or bitter;&lt;/p&gt;

&lt;p&gt;for always there will be greater and lesser persons than yourself.&lt;/p&gt;

&lt;p&gt;Enjoy your achievements as well as your plans.&lt;/p&gt;

&lt;p&gt;Keep interested in your own career, however humble;&lt;/p&gt;

&lt;p&gt;it is a real possession in the changing fortunes of time.&lt;/p&gt;

&lt;p&gt;&amp;ndash;TBC&lt;/p&gt;

&lt;h1 id=&#34;环境构建:1784e58cad9564fb26f40c083f1bce7b&#34;&gt;环境构建&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;spark 支持多个版本的Hadoop， 无论是 Apache Hadoop 还是Cloudera 的CDH， Hartonworks的。 你用什么版本的hadoop所以问题不大&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Spark runs on both Windows and UNIX-like systems (e.g. Linux, Mac OS). 所以window 就不行了，支持Unix家族的机器 Mac， Linux（Ubuntu， Redhat， Centos等）.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Spark runs on &lt;strong&gt;Java 7+&lt;/strong&gt;, &lt;strong&gt;Python 2.6+&lt;/strong&gt; and &lt;strong&gt;R 3.1+&lt;/strong&gt;.  Spark 1.6.0 uses &lt;strong&gt;Scala 2.10&lt;/strong&gt;. 所以Scala 配置 2.10.×版本的。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;我的电脑是Ubuntu 15.4 还需要安装：&lt;/p&gt;

&lt;h3 id=&#34;java:1784e58cad9564fb26f40c083f1bce7b&#34;&gt;JAVA&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;安装JDK &lt;a href=&#34;http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html&#34;&gt;下载地址&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;解压安装&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;（ubuntu系统）&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd /home/tom
$ tar -xzvf jdk-8u73-linux-x64.tar.gz       
$ sudo vim /etc/profile
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;(Centos)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;查看yum库中的Java安装包。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yum -y list java* 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;用yum安装Java环境。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yum -y install java-1.7.0-openjdk* ，//以yum库中java-1.7.0为例。 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当结果显示为Complete！即安装完毕。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;查看是否成功&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ java -version
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;至于centos/ubuntu/debian 的&lt;strong&gt;区别&lt;/strong&gt; &lt;a href=&#34;http://blog.csdn.net/educast/article/details/38315433/&#34;&gt;请看&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;scala环境:1784e58cad9564fb26f40c083f1bce7b&#34;&gt;SCALA环境&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;官网上下载，解压到任意目录&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd /opt
$ tar -xzvf scala-2.11.6.tgz
$ sudo vim /etc/profile  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同上修改SCALA路径&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;ssh-配置:1784e58cad9564fb26f40c083f1bce7b&#34;&gt;SSH 配置&lt;/h3&gt;

&lt;p&gt;所谓ssh就是ssh免密码登录服务器，其中用到了RSA加密算法。其中的细节和原理我有时间再写。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;确保安装好&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ssh：（ubuntu版）
$ sudo apt-get update
$ sudo apt-get install openssh-server
$ sudo /etc/init.d/ssh start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ssh(centos): 确认系统已经安装了SSH。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rpm –qa | grep openssh
rpm –qa | grep rsync


yum install ssh //安装SSH协议
yum install rsync //rsync是一个远程数据同步工具，可通过LAN/WAN快速同步多台主机间的文件
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;service sshd restart &amp;ndash;&amp;gt;启动服务
2. 生成并添加密钥：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    $ ssh-keygen -t rsa
    $ cat ~/.ssh/id_rsa.pub &amp;gt;&amp;gt; ~/.ssh/authorized_keys
    $ chmod 0600 ~/.ssh/authorized_keys

如果已经生成过密钥，只需执行后两行命令。
测试ssh localhost

    $ ssh localhost
    $ exit
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;hadoop-环境:1784e58cad9564fb26f40c083f1bce7b&#34;&gt;Hadoop 环境&lt;/h3&gt;

&lt;p&gt;个人觉得，单机可装可不装&amp;hellip;.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;解压hadoop2.6.0到任意目录：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd /opt
$ wget http://apache.claz.org/hadoop/common/hadoop-2.6.0/hadoop-2.6.0.tar.gz
$ tar -xzvf hadoop-2.6.0.tar.gz
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;编辑/etc/profile文件，在最后加上java环境变量：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export HADOOP_HOME=/opt/hadoop-2.6.0
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;编辑 &lt;code&gt;$HADOOP_HOME/etc/hadoop/hadoop-env.sh&lt;/code&gt;文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ vim $HADOOP_HOME/etc/hadoop/hadoop-env.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在最后加上：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export JAVA_HOME=/opt/jdk1.8.0_73/（你自己java的地址）
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;修改&lt;code&gt;configuration&lt;/code&gt;文件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd $HADOOP_HOME/etc/hadoop
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;修改&lt;code&gt;core-site.xml&lt;/code&gt;：&lt;/p&gt;

&lt;p&gt;&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;fs.default.name&lt;/name&gt;
         &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;修改&lt;code&gt;hdfs-site.xml&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;configuration&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;dfs.replication&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;1&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;


    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;dfs.name.dir&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;file:///opt/hadoop-2.6.1/dfs/name&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;


    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;dfs.data.dir&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;file:///opt/hadoop-2.6.1/dfs/data&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;第一个是dfs的备份数目，单机用1份就行，后面两个是namenode和datanode的目录。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;修改&lt;code&gt;mapred-site.xml&lt;/code&gt;：&lt;/p&gt;

&lt;p&gt;&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
        &lt;value&gt;yarn&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;修改yarn-site.xml：&lt;/p&gt;

&lt;p&gt;&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;初始化hadoop：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ $HADOOP_HOME/bin/hdfs namenode -format
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;启动&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ $HADOOP_HOME/sbin/start-all.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;停止&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ $HADOOP_HOME/sbin/stop-all.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;检查WebUI，浏览器打开端口：&lt;a href=&#34;http://localhost:8088&#34;&gt;http://localhost:8088&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;port 8088: cluster and all applications&lt;/p&gt;

&lt;p&gt;port 50070: Hadoop NameNode&lt;/p&gt;

&lt;p&gt;port 50090: Secondary NameNode&lt;/p&gt;

&lt;p&gt;port 50075: DataNode&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;hadoop运行后可使用jps命令查看,得到结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;10057 Jps
9611 ResourceManager
9451 SecondaryNameNode
9260 DataNode
9102 NameNode
9743 NodeManager
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;spark-安装方式:1784e58cad9564fb26f40c083f1bce7b&#34;&gt;SPARK 安装方式&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;源码编译&lt;/p&gt;

&lt;p&gt;需要从github上clone下来， 通过maven 或者 sbt 进行编译。读者可以到&lt;a href=&#34;http://spark.apache.org/docs/latest/&#34;&gt;官网&lt;/a&gt; 上进行下载。&lt;/p&gt;

&lt;p&gt;如果需要自己编译，那么需要配置环境（见下）， 然后通过 sbt 进行编译（见下）。当然如果觉得太麻烦可以直接下载&lt;code&gt;预编译的版本&lt;/code&gt;。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;预编译的版本&lt;/p&gt;

&lt;p&gt;Prebuilt 版本就方便多了&lt;a href=&#34;http://www.apache.org/dyn/closer.lua/spark/spark-1.6.0/spark-1.6.0-bin-hadoop2.6.tgz&#34;&gt;spark-1.6.0-bin-hadoop2.6.tgz&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;下载到制定位置&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;解压&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; tar -xzvf spark-1.6.0-bin-hadoop2.6.tgz
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;在&lt;code&gt;/etc/profile&lt;/code&gt;文件的末尾添加环境变量&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export SPARK_HOME=/opt/spark-1.6.0
export PATH=$SPARK_HOME/bin:$PATH
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;保存更新同上&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ source /etc/profil
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;脚本配置
conf目录下复制并重命名spark-env.sh.template为spark-env.sh&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ mv spark-env.sh.template spark-env.sh
$ vim spark-env.sh //进行配置
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在&lt;code&gt;spark-env.sh&lt;/code&gt;中添加：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export JAVA_HOME=/opt/jdk1.8.0_73/
export SCALA_HOME=/opt/scala-2.10.6
export SPARK_MASTER_IP=localhost
export SPARK_WORKER_MEMORY=4G 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ $SPARK_HOME/sbin/start-all.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;停止&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ $SPARK_HOME/sbin/stop-all.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;测试Spark是否安装成功：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ $SPARK_HOME/bin/run-example SparkPi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;得到结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Pi is roughly 3.14716
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;检查WebUI，浏览器打开端口：&lt;a href=&#34;http://localhost:8080&#34;&gt;http://localhost:8080&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;在shell脚本中可以看到很多信息，openjdk版本， scala版本等等。 同时可以通过web 的UI界面访问：&lt;a href=&#34;http://localhost:4040/jobs/&#34;&gt;http://localhost:4040/jobs/&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;编译工具安装-sbt:1784e58cad9564fb26f40c083f1bce7b&#34;&gt;编译工具安装 - sbt&lt;/h1&gt;

&lt;p&gt;spark 官网上推荐 maven或者sbt 进行编译scala文件。 我个人推荐用sbt，轻便简单。但是容易在安装时遇到问题（gfw）。 sbt 在之后 运行spark工程时也需要，编译打包放在spark submit 上运行&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;官网下载地址&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;http://www.scala-sbt.org/&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;运行安装包&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;创建工程:1784e58cad9564fb26f40c083f1bce7b&#34;&gt;创建工程：&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;新建工程文件夹&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;比如现在的工程名为“sparksample”。那么&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd sparksample
mkdir project
mkdir src/main/scala
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一般的工程文件结构如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;project – 工程定义文件&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;project/build.sbt – 主要的工程定义文件&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;project/build.properties – 工程，sbt以及scala版本定义&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;src/main – 你的应用代码放在这里，不同的子目录名称表示不同的编程语言（例如，src/main/scala,src/main/java)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;src/main/resources – 你想添加到jar包里的静态文件（例如日志配置文件）&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;lib_managed – 你的工程所依赖的jar文件。会在sbt更新的时候添加到该目录&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;target – 最终生成的文件存放的目录（例如，生成的thrift代码，class文件，jar文件）&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;编写build.sbt文件&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;name := &amp;quot;SparkSample&amp;quot;
version := &amp;quot;1.0&amp;quot;
scalaVersion := &amp;quot;2.10.3&amp;quot;
libraryDependencies += &amp;quot;org.apache.spark&amp;quot; %% &amp;quot;spark-core&amp;quot; % &amp;quot;1.1.1&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;这里需要注意使用的版本，scala 和spark streaming的版本是否匹配等等。&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;查看地址：
&lt;a href=&#34;http://mvnrepository.com/artifact/org.apache.spark/spark-streaming_2.10/1.4.1&#34;&gt;http://mvnrepository.com/artifact/org.apache.spark/spark-streaming_2.10/1.4.1&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;构建jar 包&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在project的文件目录下（e.g. &amp;ldquo;sparksample&amp;rdquo;）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sbt package
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;提交到spark submit&lt;/strong&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd /opt/spark-verisonnuber/bin/


./spark-submit --class &amp;quot;org.apache.spark.examples.streaming.sparksample&amp;quot; --packages org.apache.spark:spark-streaming-kafka_2.10:1.4.1 --master local[2]  /home/sherry/sparksample/target/scala-2.10/sparksample-1.0.jar 10.81.52.88:9092 tintin  
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;具体怎么写参数，请看官方：&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://spark.apache.org/docs/latest/submitting-applications.html#submitting-applications&#34;&gt;http://spark.apache.org/docs/latest/submitting-applications.html#submitting-applications&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;**注意**&lt;/code&gt;： 略坑的是， 需要将调用的包手动加入  &amp;ndash; packages  ****&lt;/p&gt;

&lt;h1 id=&#34;reference:1784e58cad9564fb26f40c083f1bce7b&#34;&gt;reference&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://blog.csdn.net/u011613321/article/details/47700211&#34;&gt;我的csdn博客&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://article.yeeyan.org/view/433044/378604&#34;&gt;工具安装 &lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.tuicool.com/articles/AJnIvq&#34;&gt;http://www.tuicool.com/articles/AJnIvq&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.scala-sbt.org/release/docs/index.html&#34;&gt;http://www.scala-sbt.org/release/docs/index.html&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.supergloo.com/fieldnotes/apache-spark-cluster-part-2-deploy-a-scala-program-to-spark-cluster/&#34;&gt;http://www.supergloo.com/fieldnotes/apache-spark-cluster-part-2-deploy-a-scala-program-to-spark-cluster/&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://segmentfault.com/a/1190000004508993#articleHeader2&#34;&gt;https://segmentfault.com/a/1190000004508993#articleHeader2&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Github contributions 故障处理</title>
      <link>http://tintinsnowy.com/2016/01/07/github-contributions-%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86</link>
      <pubDate>Thu, 07 Jan 2016 20:16:48 +0800</pubDate>
      
      <guid>http://tintinsnowy.com/2016/01/07/github-contributions-%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86</guid>
      <description>

&lt;h1 id=&#34;楔子:0b34f49b7bb7420e691a295419514e40&#34;&gt;楔子&lt;/h1&gt;

&lt;p&gt;同样用一首英文诗开场&lt;/p&gt;

&lt;h2 id=&#34;the-desiderata-part-1:0b34f49b7bb7420e691a295419514e40&#34;&gt;The Desiderata (part 1)&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;by Max Ehrmann, 1927&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Go placidly amid the noise and the haste,&lt;/p&gt;

&lt;p&gt;and remember what peace there may be in silence.&lt;/p&gt;

&lt;p&gt;As far as possible, without surrender,&lt;/p&gt;

&lt;p&gt;be on good terms with all persons.&lt;/p&gt;

&lt;p&gt;Speak your truth quietly and clearly;&lt;/p&gt;

&lt;p&gt;and listen to others,&lt;/p&gt;

&lt;p&gt;even to the dull and the ignorant;&lt;/p&gt;

&lt;p&gt;they too have their story.&lt;/p&gt;

&lt;p&gt;Avoid loud and aggressive persons;&lt;/p&gt;

&lt;p&gt;they are vexatious to the spirit.&lt;/p&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;正文&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;作为programmers， 在github上提交代码，看见commit就是每天的动力。但是有可能因为以下几种情况发生contributions无法记录&lt;/p&gt;

&lt;h2 id=&#34;什么样的contribution-github才记录:0b34f49b7bb7420e691a295419514e40&#34;&gt;什么样的Contribution Github才记录？&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;http://blog.aizhet.com/UploadFiles/2014/4/201404250819115885.png&#34; alt=&#34;green！&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;1-issues-and-pull-requests:0b34f49b7bb7420e691a295419514e40&#34;&gt;1 Issues and pull requests&lt;/h2&gt;

&lt;p&gt;同时满足以下两个条件将会被计入Contribution&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;这个操作是在一年之内。（Calendar只显示一年之内的Contribution）&lt;/li&gt;
&lt;li&gt;这个操作是针对一个独立的仓库。（在Fork的仓库中进行的操作不会被记录）&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;2-commits:0b34f49b7bb7420e691a295419514e40&#34;&gt;2 Commits&lt;/h2&gt;

&lt;p&gt;同时满足以下四个条件将会被计入Contribution&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Commits是在一年之内。（Calendar只显示一年之内的Contribution）&lt;/li&gt;
&lt;li&gt;进行Commits的用户被关联到了你的Github帐号中（使用SSH方式能够不输入帐号密码进行push，如果此时你Commit的帐号不在Github帐号列表中，就不会被计入Contribution）&lt;/li&gt;
&lt;li&gt;是在一个独立的版本库中进行Commit。（在Fork的仓库中进行Commit则不会被记录）&lt;/li&gt;
&lt;li&gt;是在这个版本库的默认分支（通常是Master）进行的Commit。（如果你在Dev分支下进行开发，你的Commit不会被计入Contribution，但是并不会丢失它们，一旦当你Merge到Master分支后，所有的Commit都会被重新计入。多人协作也是同理，只有被并入Master分支的Commit才会被计入，如果你的Commit在合并时被组长丢弃，在Github看来，你就白干了……）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;## 3 others
   &amp;gt; 附加条件：如果你Commit的仓库不是你创建的，那么至少要满足以下四条之一，才会被计入Contribution&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;你是这个仓库的协作者，或者是这个版本库的拥有组织中的一员。&lt;/li&gt;
&lt;li&gt;你fork过这个仓库。&lt;/li&gt;
&lt;li&gt;你对这个仓库发起过pull request或者issue。&lt;/li&gt;
&lt;li&gt;你对这个仓库标记了Star。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;（私有仓库的Commit也会被计入Contribution，没有这个私有仓库权限的用户将看不到这个Commit的跳转链接）&lt;/p&gt;

&lt;h2 id=&#34;contributions未被github计入的常见原因:0b34f49b7bb7420e691a295419514e40&#34;&gt;Contributions未被Github计入的常见原因&lt;/h2&gt;

&lt;p&gt;可以通过github 上的commit 历史记录进行分析。 比如我就发现commit 的用户名是之前在gerrit（另一个提交/审核代码的工具）的用户名，于是发现应该是 &lt;strong&gt;&lt;code&gt;git config设置问题&lt;/code&gt;&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;解决：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;重新设置你github 的账号和邮箱即可&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;         git config --global user.name [username]
         git config --global user.email [email]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;附录:0b34f49b7bb7420e691a295419514e40&#34;&gt;附录&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://blog.csdn.net/dss886/article/details/39271613?utm_source=tuicool&amp;amp;utm_medium=referral&#34;&gt; Github不记录Contributions的问题 &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://help.github.com/articles/why-are-my-contributions-not-showing-up-on-my-profile/&#34;&gt;help of github &lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>博客工具</title>
      <link>http://tintinsnowy.com/2015/12/27/%E5%8D%9A%E5%AE%A2%E5%B7%A5%E5%85%B7</link>
      <pubDate>Sun, 27 Dec 2015 12:40:57 +0800</pubDate>
      
      <guid>http://tintinsnowy.com/2015/12/27/%E5%8D%9A%E5%AE%A2%E5%B7%A5%E5%85%B7</guid>
      <description>

&lt;h1 id=&#34;楔子:4396d87bf52c8e0d9b74841cebec2d9e&#34;&gt;楔子&lt;/h1&gt;

&lt;p&gt;喜欢写Blog的人，会经历三个阶段。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;第一阶段，刚接触Blog，觉得很新鲜，试着选择一个免费空间来写。
  第二阶段，发现免费空间限制太多，就自己购买域名和空间，搭建独立博客。
  第三阶段，觉得独立博客的管理太麻烦，最好在保留控制权的前提下，让别人来管，自己只负责写文章。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;搭建自己的博客是有点折腾，但是会乐在其中。 考虑到技术文章会涉及到大量的英文单词，或者命令就用英文写。 概述类和心得篇就用中文。
p.s.&lt;a href=&#34;http://blog.csdn.net/u011613321&#34;&gt;&lt;code&gt;旧csdn博客地址&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;所以针对新博客搭建有几个plan：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;初期：基本功能&lt;code&gt;分类&lt;/code&gt;， &lt;code&gt;标签&lt;/code&gt;，&lt;code&gt;联系方式&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;中期：&lt;code&gt;评论&lt;/code&gt;， &lt;code&gt;网站分析&lt;/code&gt;， &lt;code&gt;rss&lt;/code&gt;， &lt;code&gt;图片托管&lt;/code&gt;，&lt;code&gt;Gallery&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;长期：和顶级域名分离，blog分&lt;code&gt;En&lt;/code&gt; 和&lt;code&gt;Ch&lt;/code&gt;， 兴许还有&lt;code&gt;De&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;初期已经差不多完成了，继续折腾。&lt;/p&gt;

&lt;h1 id=&#34;disqus-添加评论功能:4396d87bf52c8e0d9b74841cebec2d9e&#34;&gt;Disqus 添加评论功能&lt;/h1&gt;

&lt;p&gt;在很久以前评论还是属于博客主的资源，在搬迁的过程中文章评论一个不能丢。数据存在网站数据中。 近年网络社交兴起博客主为了吸引访客互动会使用第三方网站托管评论。Disqus就是这样的第三方社会化评论平台，注册Disqus后你会发现：这根本就是个社交平台啊！    所以同志们在你的博客上评论后，你登陆Disqus 后台，会看见&lt;code&gt;评论&lt;/code&gt;， &lt;code&gt;推荐&lt;/code&gt;，&lt;code&gt;喜爱&lt;/code&gt; 等等。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;个人信息&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Profile: Full Name(昵称，任意取)&lt;/li&gt;
&lt;li&gt;Avatar:头像支持本地上传，网络图像URL，所在网站的默认头像，Facebook头像，Twitter头像。当然，后两者需要绑定对应的社交帐号。&lt;/li&gt;
&lt;li&gt;Serives:第三方帐号的管理界面，可进行绑定、解绑社交帐号，目前所支持的除了三大社交网络帐号，还有Yahoo! 已绑定的帐号名在Disqus评论系统上是公开的。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;账户&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Username： &lt;code&gt;唯一&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Email ： 邮箱进行验证。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;开始加入Disqus&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;点击&lt;a href=&#34;https://disqus.com/admin/create/&#34;&gt;注册&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;这里注册的&lt;code&gt;Site name&lt;/code&gt; 就是我们将要用到的&lt;code&gt;shortname&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;注册后你会得到一个 &lt;code&gt;sherrysblog.disqus.com/&lt;/code&gt;东西，其中&lt;code&gt;sherrysblog&lt;/code&gt;就是我的&lt;code&gt;shortname&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;在我的账户内&lt;code&gt;https://sherrysblog.disqus.com/admin/settings/general/&lt;/code&gt;
可以进行个人网站的设置， 语言设置等。&lt;/li&gt;
&lt;li&gt;done.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;缺点&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;有收费的可能&lt;/li&gt;
&lt;li&gt;不支持国内社交账号&lt;/li&gt;
&lt;li&gt;没有私信功能。&lt;/li&gt;
&lt;li&gt;可能被墙&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;google-analytics-网站分析:4396d87bf52c8e0d9b74841cebec2d9e&#34;&gt;Google Analytics 网站分析&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;https://cdn.mycommerce.com/images/mycommercecorner/thumbnails/Google_Analytics/GASample.jpg&#34; alt=&#34;google analytics&#34; /&gt;
 这其实是个很强大的工具，但是想想挺可怕的。因为如果作为商业用户，会实时监控我们大众用户的行为，这就是所谓大数据%&amp;gt;_&amp;lt;%&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;注册 google analytics的账号&lt;/li&gt;
&lt;li&gt;填写自己blog 的位置&lt;code&gt;tintinsnowy.com&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;将Tracing code 粘贴到自己&lt;code&gt;config.toml&lt;/code&gt;中。&lt;/li&gt;
&lt;li&gt;通过google analytics 后台账号查看用户访问情况。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&amp;mdash;&amp;ndash;&lt;strong&gt;TBC&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;writen by Sherry&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>