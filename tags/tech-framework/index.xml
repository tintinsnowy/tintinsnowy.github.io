<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tech Framework on Sherry&#39;s blog</title>
    <link>http://tintinsnowy.com/tags/tech-framework/</link>
    <description>Recent content in Tech Framework on Sherry&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 14 Mar 2016 09:06:34 +0800</lastBuildDate>
    <atom:link href="http://tintinsnowy.com/tags/tech-framework/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Spark-多节点集群配置</title>
      <link>http://tintinsnowy.com/2016/03/14/spark-%E5%A4%9A%E8%8A%82%E7%82%B9%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE</link>
      <pubDate>Mon, 14 Mar 2016 09:06:34 +0800</pubDate>
      
      <guid>http://tintinsnowy.com/2016/03/14/spark-%E5%A4%9A%E8%8A%82%E7%82%B9%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE</guid>
      <description>

&lt;h1 id=&#34;楔子:d7f0df7a88d75431aefaa0c11af54130&#34;&gt;楔子&lt;/h1&gt;

&lt;p&gt;这次完全拿到的是裸机，所以从零开始配置。其实集群和单节点差不多，见我前面的blog&lt;/p&gt;

&lt;h2 id=&#34;本机配置:d7f0df7a88d75431aefaa0c11af54130&#34;&gt;本机配置&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;Centos 5.8&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;4 cores 8G&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&#34;节点布置-masters-slaves:d7f0df7a88d75431aefaa0c11af54130&#34;&gt;节点布置 Masters&amp;amp;Slaves&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Master   119.254.168.33&lt;/li&gt;
&lt;li&gt;Slaves1  119.254.168.34&lt;/li&gt;
&lt;li&gt;Slaves2  119.254.168.36&lt;/li&gt;
&lt;li&gt;Slaves3  119.254.168.38&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;环境配置-environment:d7f0df7a88d75431aefaa0c11af54130&#34;&gt;环境配置 Environment&lt;/h1&gt;

&lt;h3 id=&#34;java-环境:d7f0df7a88d75431aefaa0c11af54130&#34;&gt;JAVA 环境&lt;/h3&gt;

&lt;p&gt;见&lt;code&gt;Apache Spark[单节点]安装和环境配置&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&#34;scala-环境:d7f0df7a88d75431aefaa0c11af54130&#34;&gt;SCALA 环境&lt;/h3&gt;

&lt;p&gt;见&lt;code&gt;Apache Spark[单节点]安装和环境配置&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&#34;ssh-配置:d7f0df7a88d75431aefaa0c11af54130&#34;&gt;SSH 配置&lt;/h3&gt;

&lt;p&gt;背景：搭建Hadoop环境需要设置无密码登陆，所谓无密码登陆其实是指通过证书认证的方式登陆 ，使用一种被称为&amp;rdquo;公私钥&amp;rdquo;(RSA)认证的方式来进行ssh登录。
在linux系统中,ssh是远程登录的默认工具,因为该工具的协议使用了RSA/DSA的加密算法.该工具做linux系统的远程管理是非常安全的。&lt;/p&gt;

&lt;p&gt;所谓ssh就是ssh免密码登录服务器，其中用到了RSA加密算法。其中的细节和原理我有时间再写。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;确保安装好&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ssh：（ubuntu版）
$ sudo apt-get update
$ sudo apt-get install openssh-server
$ sudo /etc/init.d/ssh start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ssh(centos): 确认系统已经安装了SSH。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rpm –qa | grep openssh
rpm –qa | grep rsync


yum install ssh //安装SSH协议
yum install rsync //rsync是一个远程数据同步工具，可通过LAN/WAN快速同步多台主机间的文件
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;service sshd restart &amp;ndash;&amp;gt;启动服务
2. 生成并添加密钥：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    $ ssh-keygen -t rsa
    $ cat ~/.ssh/id_rsa.pub &amp;gt;&amp;gt; ~/.ssh/authorized_keys
    $ chmod 0600 ~/.ssh/authorized_keys
    service sshd restart //一般修改过都需要重启服务
如果已经生成过密钥，只需执行后两行命令。
测试ssh localhost

    $ ssh localhost
    $ exit

查看端口：是否打开

    netstat -anp |grep ssh
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;hadoop-cluster-installation:d7f0df7a88d75431aefaa0c11af54130&#34;&gt;Hadoop cluster Installation&lt;/h1&gt;

&lt;p&gt;基本和前面相同&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;修改hdfs-site.xml&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;configuration&amp;gt;
    &amp;lt;property&amp;gt;
         &amp;lt;name&amp;gt;dfs.namenode.secondary.http-address&amp;lt;/name&amp;gt;
         &amp;lt;value&amp;gt;kexinyun1:9001&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;


    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;dfs.namenode.name.dir&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;file:///opt/hadoop-2.6.1/dfs/name&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;


    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;dfs.datanode.data.dir&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;file:///opt/hadoop-2.6.1/dfs/data&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;


    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;dfs.replication&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;3&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;


    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;dfs.webhdfs.enabled&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;/configuration&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;mapred-site.xml&lt;/p&gt;

&lt;p&gt;&lt;configuration&gt;&lt;br /&gt;
    &lt;property&gt;&lt;br /&gt;
&lt;name&gt;mapreduce.framework.name&lt;/name&gt;&lt;br /&gt;
&lt;value&gt;yarn&lt;/value&gt;&lt;br /&gt;
    &lt;/property&gt;&lt;br /&gt;
&lt;property&gt;&lt;br /&gt;
    &lt;name&gt;mapreduce.jobtracker.http.address&lt;/name&gt;&lt;br /&gt;
    &lt;value&gt;nameNode:50030&lt;/value&gt;&lt;br /&gt;
&lt;/property&gt;&lt;br /&gt;
&lt;property&gt;&lt;br /&gt;
    &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;&lt;br /&gt;
    &lt;value&gt;nameNode:10020&lt;/value&gt;&lt;br /&gt;
&lt;/property&gt;&lt;br /&gt;
&lt;property&gt;&lt;br /&gt;
    &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;&lt;br /&gt;
    &lt;value&gt;nameNode:19888&lt;/value&gt;&lt;br /&gt;
&lt;/property&gt;&lt;br /&gt;
&lt;/configuration&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;yarn&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;configuration&amp;gt;
    &amp;lt;!-- Site specific YARN configuration properties --&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;yarn.nodemanager.aux-services&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;mapreduce_shuffle&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;yarn.resourcemanager.address&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;kexinyun1:8032&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;yarn.resourcemanager.scheduler.address&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;kexinyun1:8030&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;yarn.resourcemanager.resource-tracker.address&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;kexinyun1:8031&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;yarn.resourcemanager.admin.address&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;kexinyun1:8033&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;yarn.resourcemanager.webapp.address&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;kexinyun1:8088&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;slaves 文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;119.254.168.38 //(slaves3)
119.254.168.36 //(slaves2)
119.254.168.34 //(slaves1)
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;vi hadoop-env.sh&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export JAVA_HOME=your java home
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;vi yarn-env.sh&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export JAVA_HOME=your java home   
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;格式化（同以前）&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;启动/停止&lt;/p&gt;

&lt;p&gt;查看：jsp&lt;/p&gt;

&lt;p&gt;访问:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://ip:9001/&#34;&gt;http://ip:9001/&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;spark-cluster-installation:d7f0df7a88d75431aefaa0c11af54130&#34;&gt;Spark Cluster Installation&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;基本同单节点类似&lt;/li&gt;

&lt;li&gt;&lt;p&gt;文件配置部分&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export SCALA_HOME=/opt/scala-2.11.4
export JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.101.x86_64/
export SPARK_HOME=/opt/spark
export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop
export SPARK_JAR=/opt/spark/lib/spark-assembly-1.6.1-hadoop2.6.0.jar
export SPARK_MASTER_IP=localhost
export SPARK_MASTER_PORT=7077
export SPARK_WORKER_CORES=1
export SPARK_WORKER_INSTANCES=1
export SPARK_WORKER_MEMORY=1g
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;启动&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$SPARK_HOME/sbin/start-all.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;problem:d7f0df7a88d75431aefaa0c11af54130&#34;&gt;problem&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;http://www.2cto.com/os/201209/155681.html&#34;&gt;http://www.2cto.com/os/201209/155681.html&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;english-version:d7f0df7a88d75431aefaa0c11af54130&#34;&gt;english version&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;http://pingax.com/install-hadoop2-6-0-on-ubuntu/&#34;&gt;http://pingax.com/install-hadoop2-6-0-on-ubuntu/&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;reference:d7f0df7a88d75431aefaa0c11af54130&#34;&gt;Reference&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://www.cnblogs.com/lanxuezaipiao/p/3525554.html&#34;&gt;http://www.cnblogs.com/lanxuezaipiao/p/3525554.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://pingax.com/install-hadoop2-6-0-on-ubuntu/&#34;&gt;http://pingax.com/install-hadoop2-6-0-on-ubuntu/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/ClusterSetup.html&#34;&gt;http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/ClusterSetup.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://blog.csdn.net/greensurfer/article/details/39450369&#34;&gt;http://blog.csdn.net/greensurfer/article/details/39450369&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Apache Spark[单节点]安装和环境配置</title>
      <link>http://tintinsnowy.com/2016/01/08/apache-spark%E5%8D%95%E8%8A%82%E7%82%B9%E5%AE%89%E8%A3%85%E5%92%8C%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE</link>
      <pubDate>Fri, 08 Jan 2016 10:36:16 +0800</pubDate>
      
      <guid>http://tintinsnowy.com/2016/01/08/apache-spark%E5%8D%95%E8%8A%82%E7%82%B9%E5%AE%89%E8%A3%85%E5%92%8C%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE</guid>
      <description>

&lt;h1 id=&#34;楔子:1784e58cad9564fb26f40c083f1bce7b&#34;&gt;楔子&lt;/h1&gt;

&lt;h2 id=&#34;the-desiderata-part-2:1784e58cad9564fb26f40c083f1bce7b&#34;&gt;The Desiderata (part 2)&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;by Max Ehrmann, 1927&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;If you compare yourself with others,&lt;/p&gt;

&lt;p&gt;you may become vain or bitter;&lt;/p&gt;

&lt;p&gt;for always there will be greater and lesser persons than yourself.&lt;/p&gt;

&lt;p&gt;Enjoy your achievements as well as your plans.&lt;/p&gt;

&lt;p&gt;Keep interested in your own career, however humble;&lt;/p&gt;

&lt;p&gt;it is a real possession in the changing fortunes of time.&lt;/p&gt;

&lt;p&gt;&amp;ndash;TBC&lt;/p&gt;

&lt;h1 id=&#34;环境构建:1784e58cad9564fb26f40c083f1bce7b&#34;&gt;环境构建&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;spark 支持多个版本的Hadoop， 无论是 Apache Hadoop 还是Cloudera 的CDH， Hartonworks的。 你用什么版本的hadoop所以问题不大&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Spark runs on both Windows and UNIX-like systems (e.g. Linux, Mac OS). 所以window 就不行了，支持Unix家族的机器 Mac， Linux（Ubuntu， Redhat， Centos等）.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Spark runs on &lt;strong&gt;Java 7+&lt;/strong&gt;, &lt;strong&gt;Python 2.6+&lt;/strong&gt; and &lt;strong&gt;R 3.1+&lt;/strong&gt;.  Spark 1.6.0 uses &lt;strong&gt;Scala 2.10&lt;/strong&gt;. 所以Scala 配置 2.10.×版本的。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;我的电脑是Ubuntu 15.4 还需要安装：&lt;/p&gt;

&lt;h3 id=&#34;java:1784e58cad9564fb26f40c083f1bce7b&#34;&gt;JAVA&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;安装JDK &lt;a href=&#34;http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html&#34;&gt;下载地址&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;解压安装&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;（ubuntu系统）&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd /home/tom
$ tar -xzvf jdk-8u73-linux-x64.tar.gz       
$ sudo vim /etc/profile
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;(Centos)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;查看yum库中的Java安装包。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yum -y list java* 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;用yum安装Java环境。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;yum -y install java-1.7.0-openjdk* ，//以yum库中java-1.7.0为例。 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当结果显示为Complete！即安装完毕。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;查看是否成功&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ java -version
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;至于centos/ubuntu/debian 的&lt;strong&gt;区别&lt;/strong&gt; &lt;a href=&#34;http://blog.csdn.net/educast/article/details/38315433/&#34;&gt;请看&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;scala环境:1784e58cad9564fb26f40c083f1bce7b&#34;&gt;SCALA环境&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;官网上下载，解压到任意目录&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd /opt
$ tar -xzvf scala-2.11.6.tgz
$ sudo vim /etc/profile  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同上修改SCALA路径&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;ssh-配置:1784e58cad9564fb26f40c083f1bce7b&#34;&gt;SSH 配置&lt;/h3&gt;

&lt;p&gt;所谓ssh就是ssh免密码登录服务器，其中用到了RSA加密算法。其中的细节和原理我有时间再写。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;确保安装好&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ssh：（ubuntu版）
$ sudo apt-get update
$ sudo apt-get install openssh-server
$ sudo /etc/init.d/ssh start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ssh(centos): 确认系统已经安装了SSH。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rpm –qa | grep openssh
rpm –qa | grep rsync


yum install ssh //安装SSH协议
yum install rsync //rsync是一个远程数据同步工具，可通过LAN/WAN快速同步多台主机间的文件
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;service sshd restart &amp;ndash;&amp;gt;启动服务
2. 生成并添加密钥：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    $ ssh-keygen -t rsa
    $ cat ~/.ssh/id_rsa.pub &amp;gt;&amp;gt; ~/.ssh/authorized_keys
    $ chmod 0600 ~/.ssh/authorized_keys

如果已经生成过密钥，只需执行后两行命令。
测试ssh localhost

    $ ssh localhost
    $ exit
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;hadoop-环境:1784e58cad9564fb26f40c083f1bce7b&#34;&gt;Hadoop 环境&lt;/h3&gt;

&lt;p&gt;个人觉得，单机可装可不装&amp;hellip;.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;解压hadoop2.6.0到任意目录：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd /opt
$ wget http://apache.claz.org/hadoop/common/hadoop-2.6.0/hadoop-2.6.0.tar.gz
$ tar -xzvf hadoop-2.6.0.tar.gz
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;编辑/etc/profile文件，在最后加上java环境变量：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export HADOOP_HOME=/opt/hadoop-2.6.0
export HADOOP_INSTALL=$HADOOP_HOME
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;编辑 &lt;code&gt;$HADOOP_HOME/etc/hadoop/hadoop-env.sh&lt;/code&gt;文件&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ vim $HADOOP_HOME/etc/hadoop/hadoop-env.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在最后加上：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export JAVA_HOME=/opt/jdk1.8.0_73/（你自己java的地址）
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;修改&lt;code&gt;configuration&lt;/code&gt;文件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd $HADOOP_HOME/etc/hadoop
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;修改&lt;code&gt;core-site.xml&lt;/code&gt;：&lt;/p&gt;

&lt;p&gt;&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;fs.default.name&lt;/name&gt;
         &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;修改&lt;code&gt;hdfs-site.xml&lt;/code&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;configuration&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;dfs.replication&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;1&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;


    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;dfs.name.dir&amp;lt;/name&amp;gt;
    &amp;lt;value&amp;gt;file:///opt/hadoop-2.6.1/dfs/name&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;


    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;dfs.data.dir&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;file:///opt/hadoop-2.6.1/dfs/data&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;第一个是dfs的备份数目，单机用1份就行，后面两个是namenode和datanode的目录。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;修改&lt;code&gt;mapred-site.xml&lt;/code&gt;：&lt;/p&gt;

&lt;p&gt;&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
        &lt;value&gt;yarn&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;修改yarn-site.xml：&lt;/p&gt;

&lt;p&gt;&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;初始化hadoop：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ $HADOOP_HOME/bin/hdfs namenode -format
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;启动&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ $HADOOP_HOME/sbin/start-all.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;停止&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ $HADOOP_HOME/sbin/stop-all.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;检查WebUI，浏览器打开端口：&lt;a href=&#34;http://localhost:8088&#34;&gt;http://localhost:8088&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;port 8088: cluster and all applications&lt;/p&gt;

&lt;p&gt;port 50070: Hadoop NameNode&lt;/p&gt;

&lt;p&gt;port 50090: Secondary NameNode&lt;/p&gt;

&lt;p&gt;port 50075: DataNode&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;hadoop运行后可使用jps命令查看,得到结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;10057 Jps
9611 ResourceManager
9451 SecondaryNameNode
9260 DataNode
9102 NameNode
9743 NodeManager
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;spark-安装方式:1784e58cad9564fb26f40c083f1bce7b&#34;&gt;SPARK 安装方式&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;源码编译&lt;/p&gt;

&lt;p&gt;需要从github上clone下来， 通过maven 或者 sbt 进行编译。读者可以到&lt;a href=&#34;http://spark.apache.org/docs/latest/&#34;&gt;官网&lt;/a&gt; 上进行下载。&lt;/p&gt;

&lt;p&gt;如果需要自己编译，那么需要配置环境（见下）， 然后通过 sbt 进行编译（见下）。当然如果觉得太麻烦可以直接下载&lt;code&gt;预编译的版本&lt;/code&gt;。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;预编译的版本&lt;/p&gt;

&lt;p&gt;Prebuilt 版本就方便多了&lt;a href=&#34;http://www.apache.org/dyn/closer.lua/spark/spark-1.6.0/spark-1.6.0-bin-hadoop2.6.tgz&#34;&gt;spark-1.6.0-bin-hadoop2.6.tgz&lt;/a&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;下载到制定位置&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;解压&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; tar -xzvf spark-1.6.0-bin-hadoop2.6.tgz
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;在&lt;code&gt;/etc/profile&lt;/code&gt;文件的末尾添加环境变量&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export SPARK_HOME=/opt/spark-1.6.0
export PATH=$SPARK_HOME/bin:$PATH
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;保存更新同上&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ source /etc/profil
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;脚本配置
conf目录下复制并重命名spark-env.sh.template为spark-env.sh&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ mv spark-env.sh.template spark-env.sh
$ vim spark-env.sh //进行配置
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在&lt;code&gt;spark-env.sh&lt;/code&gt;中添加：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;export JAVA_HOME=/opt/jdk1.8.0_73/
export SCALA_HOME=/opt/scala-2.10.6
export SPARK_MASTER_IP=localhost
export SPARK_WORKER_MEMORY=4G 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ $SPARK_HOME/sbin/start-all.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;停止&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ $SPARK_HOME/sbin/stop-all.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;测试Spark是否安装成功：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ $SPARK_HOME/bin/run-example SparkPi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;得到结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Pi is roughly 3.14716
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;检查WebUI，浏览器打开端口：&lt;a href=&#34;http://localhost:8080&#34;&gt;http://localhost:8080&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;在shell脚本中可以看到很多信息，openjdk版本， scala版本等等。 同时可以通过web 的UI界面访问：&lt;a href=&#34;http://localhost:4040/jobs/&#34;&gt;http://localhost:4040/jobs/&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;编译工具安装-sbt:1784e58cad9564fb26f40c083f1bce7b&#34;&gt;编译工具安装 - sbt&lt;/h1&gt;

&lt;p&gt;spark 官网上推荐 maven或者sbt 进行编译scala文件。 我个人推荐用sbt，轻便简单。但是容易在安装时遇到问题（gfw）。 sbt 在之后 运行spark工程时也需要，编译打包放在spark submit 上运行&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;官网下载地址&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;http://www.scala-sbt.org/&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;运行安装包&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;创建工程:1784e58cad9564fb26f40c083f1bce7b&#34;&gt;创建工程：&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;新建工程文件夹&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;比如现在的工程名为“sparksample”。那么&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd sparksample
mkdir project
mkdir src/main/scala
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一般的工程文件结构如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;project – 工程定义文件&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;project/build.sbt – 主要的工程定义文件&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;project/build.properties – 工程，sbt以及scala版本定义&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;src/main – 你的应用代码放在这里，不同的子目录名称表示不同的编程语言（例如，src/main/scala,src/main/java)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;src/main/resources – 你想添加到jar包里的静态文件（例如日志配置文件）&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;lib_managed – 你的工程所依赖的jar文件。会在sbt更新的时候添加到该目录&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;target – 最终生成的文件存放的目录（例如，生成的thrift代码，class文件，jar文件）&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;编写build.sbt文件&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;name := &amp;quot;SparkSample&amp;quot;
version := &amp;quot;1.0&amp;quot;
scalaVersion := &amp;quot;2.10.3&amp;quot;
libraryDependencies += &amp;quot;org.apache.spark&amp;quot; %% &amp;quot;spark-core&amp;quot; % &amp;quot;1.1.1&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;这里需要注意使用的版本，scala 和spark streaming的版本是否匹配等等。&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;查看地址：
&lt;a href=&#34;http://mvnrepository.com/artifact/org.apache.spark/spark-streaming_2.10/1.4.1&#34;&gt;http://mvnrepository.com/artifact/org.apache.spark/spark-streaming_2.10/1.4.1&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;构建jar 包&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在project的文件目录下（e.g. &amp;ldquo;sparksample&amp;rdquo;）&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sbt package
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;提交到spark submit&lt;/strong&gt;：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd /opt/spark-verisonnuber/bin/


./spark-submit --class &amp;quot;org.apache.spark.examples.streaming.sparksample&amp;quot; --packages org.apache.spark:spark-streaming-kafka_2.10:1.4.1 --master local[2]  /home/sherry/sparksample/target/scala-2.10/sparksample-1.0.jar 10.81.52.88:9092 tintin  
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;具体怎么写参数，请看官方：&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://spark.apache.org/docs/latest/submitting-applications.html#submitting-applications&#34;&gt;http://spark.apache.org/docs/latest/submitting-applications.html#submitting-applications&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;**注意**&lt;/code&gt;： 略坑的是， 需要将调用的包手动加入  &amp;ndash; packages  ****&lt;/p&gt;

&lt;h1 id=&#34;reference:1784e58cad9564fb26f40c083f1bce7b&#34;&gt;reference&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://blog.csdn.net/u011613321/article/details/47700211&#34;&gt;我的csdn博客&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://article.yeeyan.org/view/433044/378604&#34;&gt;工具安装 &lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.tuicool.com/articles/AJnIvq&#34;&gt;http://www.tuicool.com/articles/AJnIvq&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.scala-sbt.org/release/docs/index.html&#34;&gt;http://www.scala-sbt.org/release/docs/index.html&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://www.supergloo.com/fieldnotes/apache-spark-cluster-part-2-deploy-a-scala-program-to-spark-cluster/&#34;&gt;http://www.supergloo.com/fieldnotes/apache-spark-cluster-part-2-deploy-a-scala-program-to-spark-cluster/&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://segmentfault.com/a/1190000004508993#articleHeader2&#34;&gt;https://segmentfault.com/a/1190000004508993#articleHeader2&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Apache Spark-初识</title>
      <link>http://tintinsnowy.com/2015/12/29/apache-spark-%E5%88%9D%E8%AF%86</link>
      <pubDate>Tue, 29 Dec 2015 22:54:05 +0800</pubDate>
      
      <guid>http://tintinsnowy.com/2015/12/29/apache-spark-%E5%88%9D%E8%AF%86</guid>
      <description>

&lt;h1 id=&#34;楔子:146e0fea47803fba4b54094cf148e79e&#34;&gt;楔子&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;Every day I remind myself that my inner and outer life are based on the labors of other men,living and dead,and that I must exert myself in order to give in the same measure as I have received and am still receiving.
                                                   &amp;mdash;-Albert Einstein&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;apache-spark:146e0fea47803fba4b54094cf148e79e&#34;&gt;Apache Spark&lt;/h1&gt;

&lt;p&gt;了解一项新技术的最好方式就是看&lt;a href=&#34;http://spark.apache.org/&#34;&gt;官网+源码+文档&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Apache Spark is a fast and general &lt;code&gt;engine&lt;/code&gt; for large-scale data processing.
它是一种快速、通用的大数据分析引擎。spark 是集批处理、实时流处理（spark streaming）、交互式查询（spark SQL）、图计算（GraphX）于一体的。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://qinxuye.me/static/uploads/spark.png&#34; alt=&#34;structure of spark&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;spark-core:146e0fea47803fba4b54094cf148e79e&#34;&gt;Spark core&lt;/h1&gt;

&lt;p&gt;Spark Core是一个基本引擎，用于大规模并行和分布式数据处理。它主要负责：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;内存管理和故障恢复&lt;/li&gt;
&lt;li&gt;在集群上安排、分布和监控作业&lt;/li&gt;
&lt;li&gt;和存储系统进行交互&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Spark引入了一个称为弹性分布式数据集（RDD，Resilient Distributed Dataset）的概念，它是一个不可变的、容错的、分布式对象集合，我们可以并行的操作这个集合。RDD可以包含任何类型的对象，它在加载外部数据集或者从驱动应用程序分发集合时创建。&lt;/p&gt;

&lt;p&gt;RDD支持两种操作类型：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;转换是一种操作（例如映射、过滤、联接、联合等等），它在一个RDD上执行操作，然后创建一个新的RDD来保存结果。&lt;/li&gt;
&lt;li&gt;行动是一种操作（例如归并、计数、第一等等），它在一个RDD上执行某种计算，然后将结果返回。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在Spark中，转换是“懒惰”(lazy)的，也就是说它们不会立刻计算出结果。相反，它们只是“记住”要执行的操作以及要操作的数据集（例如文件）。只有当行为被调用时，转换才会真正的进行计算，并将结果返回给驱动器程序。这种设计让Spark运行得更有效率。例如，如果一个大文件要通过各种方式进行转换操作，并且文件被传递给第一个行为，那么Spark只会处理文件的第一行内容并将结果返回，而不会处理整个文件。&lt;/p&gt;

&lt;p&gt;默认情况下，当你在经过转换的RDD上运行一个行为时，这个RDD有可能会被重新计算。然而，你也可以通过使用持久化或者缓存的方法，将一个RDD持久化从在尽可能多地在内存中而然后才往磁盘中去写，这样，Spark就会在集群上保留这些元素，当你下一次查询它时，查询速度会快很多。&lt;/p&gt;

&lt;h1 id=&#34;sparksql:146e0fea47803fba4b54094cf148e79e&#34;&gt;SparkSQL&lt;/h1&gt;

&lt;pre&gt;&lt;code&gt; // sc is an existing SparkContext.启动shell后sc is available
 val sqlContext = new org.apache.spark.sql.hive.HiveContext(sc)
 
 sqlContext.sql(&amp;quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING)&amp;quot;)
 sqlContext.sql(&amp;quot;LOAD DATA LOCAL INPATH     examples/src/main/resources/kv1.txt&#39; INTO TABLE src&amp;quot;)
 
 // Queries are expressed in HiveQL
 sqlContext.sql(&amp;quot;FROM src SELECT key, value&amp;quot;).collect().foreach(println)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;或者从Hdfs中读入数据表：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;   val sqlContext = new SQLContext(sc)
   df = sqlContext.read.format(&amp;quot;com.databricks.spark.csv&amp;quot;).options(opts).load(hdfs
          + &amp;quot;/s_user&amp;quot;)  //将hdfs dfs ls /s_user/*下所有文件读入形成dataframe的格式

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以通过JDBC API将Spark数据集暴露出去，而且还可以用传统的BI和可视化工具在Spark数据上执行类似SQL的查询（比如Tableau）。也可以从外部（hive，文件，hdfs）上读入文件，进行sql语句的查询。&lt;/p&gt;

&lt;p&gt;关于sparksql 的使用会在后面几篇文章中重点介绍。&lt;/p&gt;

&lt;h1 id=&#34;spark-streaming:146e0fea47803fba4b54094cf148e79e&#34;&gt;Spark Streaming&lt;/h1&gt;

&lt;p&gt;Spark Streaming支持对流数据的实时处理，例如产品环境web服务器的日志文件（例如Apache Flume和HDFS/S3）、诸如Twitter的社交媒体以及像Kafka那样的各种各样的消息队列。在这背后，Spark Streaming会接收输入数据，然后将其分为不同的批次，接下来Spark引擎来处理这些批次，并根据批次中的结果，生成最终的流。整个过程如下所示。
&lt;img src=&#34;http://img1.imgtn.bdimg.com/it/u=1793884862,2093564393&amp;amp;fm=21&amp;amp;gp=0.jpg&#34; alt=&#34;the process&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;mllib:146e0fea47803fba4b54094cf148e79e&#34;&gt;MLlib&lt;/h1&gt;

&lt;p&gt;MLlib是一个机器学习库，它提供了各种各样的算法，这些算法用来在集群上针对分类、回归、聚类、协同过滤等（可以在 machine learning 上查看Toptal的文章，来获取更过的信息）。其中一些算法也可以应用到流数据上，例如使用普通最小二乘法或者K均值聚类（还有更多）来计算线性回归。Apache Mahout（一个针对Hadoop的机器学习库）已经脱离MapReduce，转而加入Spark MLlib。&lt;/p&gt;

&lt;h1 id=&#34;graphx:146e0fea47803fba4b54094cf148e79e&#34;&gt;GraphX&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;http://ww2.sinaimg.cn/mw690/6941baebjw1ev5gmlwad0j20o008875e.jpg&#34; alt=&#34;GraphX&#34; /&gt;&lt;/p&gt;

&lt;p&gt;GraphX是一个库，用来处理图，执行基于图的并行操作。它针对ETL、探索性分析和迭代图计算提供了统一的工具。除了针对图处理的内置操作，GraphX还提供了一个库，用于通用的图算法，例如PageRank。&lt;/p&gt;

&lt;h1 id=&#34;spark特性:146e0fea47803fba4b54094cf148e79e&#34;&gt;Spark特性&lt;/h1&gt;

&lt;p&gt;Spark通过在数据处理过程中成本更低的洗牌（Shuffle）方式，将MapReduce提升到一个更高的层次。利用内存数据存储和接近实时的处理能力，Spark比其他的大数据处理技术的性能要快很多倍。&lt;/p&gt;

&lt;p&gt;Spark还支持大数据查询的延迟计算，这可以帮助优化大数据处理流程中的处理步骤。Spark还提供高级的API以提升开发者的生产力，除此之外还为大数据解决方案提供一致的体系架构模型。&lt;/p&gt;

&lt;p&gt;Spark将中间结果保存在内存中而不是将其写入磁盘，当需要多次处理同一数据集时，这一点特别实用。Spark的设计初衷就是既可以在内存中又可以在磁盘上工作的执行引擎。当内存中的数据不适用时，Spark操作符就会执行外部操作。Spark可以用于处理大于集群内存容量总和的数据集。&lt;/p&gt;

&lt;p&gt;Spark会尝试在内存中存储尽可能多的数据然后将其写入磁盘。它可以将某个数据集的一部分存入内存而剩余部分存入磁盘。开发者需要根据数据和用例评估对内存的需求。Spark的性能优势得益于这种内存中的数据存储。&lt;/p&gt;

&lt;h1 id=&#34;spark的其他特性包括:146e0fea47803fba4b54094cf148e79e&#34;&gt;Spark的其他特性包括：&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;支持比Map和Reduce更多的函数。&lt;/li&gt;
&lt;li&gt;优化任意操作算子图（operator graphs）。&lt;/li&gt;
&lt;li&gt;可以帮助优化整体数据处理流程的大数据查询的延迟计算。&lt;/li&gt;
&lt;li&gt;提供简明、一致的Scala，Java和Python API。&lt;/li&gt;
&lt;li&gt;支持语言：scala、java、Python、R&lt;/li&gt;
&lt;li&gt;提供交互式Scala和Python Shell。目前暂不支持Java。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;spark体系架构:146e0fea47803fba4b54094cf148e79e&#34;&gt;Spark体系架构&lt;/h1&gt;

&lt;p&gt;&lt;img src=&#34;http://cdn4.infoqstatic.com/statics_s1_20151224-0209/resource/articles/apache-spark-introduction/zh/resources/0304082.png&#34; alt=&#34;3大体系&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;1-数据存储:146e0fea47803fba4b54094cf148e79e&#34;&gt;1.数据存储&lt;/h2&gt;

&lt;p&gt;支持从hadoop存储框架里的任何读入包括HDFS，HBase，Cassandra，hive。同时也可支持本地文件，输入&lt;/p&gt;

&lt;h2 id=&#34;2-api:146e0fea47803fba4b54094cf148e79e&#34;&gt;2.API&lt;/h2&gt;

&lt;p&gt;通过API 开发者可以制定任何基于spark的应用。API的文档在spark 官方网站中&lt;/p&gt;

&lt;h2 id=&#34;3-资源调度:146e0fea47803fba4b54094cf148e79e&#34;&gt;3.资源调度&lt;/h2&gt;

&lt;p&gt;spark可以安装在单机pc下，也支持部署在yarn这样的分布式计算框架下&lt;/p&gt;

&lt;h1 id=&#34;reference:146e0fea47803fba4b54094cf148e79e&#34;&gt;Reference&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://www.infoq.com/cn/articles/apache-spark-introduction&#34;&gt;用Apache Spark进行大数据处理&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://blog.jobbole.com/89446/&#34;&gt;Apache Spark 入门简介&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://qinxuye.me/article/dpark-source-code-analysis/&#34;&gt;Spark源码剖析&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>