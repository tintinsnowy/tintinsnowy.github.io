<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Mining on Sherry&#39;s blog</title>
    <link>uild/tags/data-mining/</link>
    <description>Recent content in Data Mining on Sherry&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 13 May 2018 09:57:55 +0100</lastBuildDate>
    
	<atom:link href="uild/tags/data-mining/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Feature Selection for unsupervised Learning with R and Python</title>
      <link>uild/2018/05/13/feature-selection-for-unsupervised-learning-with-r-and-python/</link>
      <pubDate>Sun, 13 May 2018 09:57:55 +0100</pubDate>
      
      <guid>uild/2018/05/13/feature-selection-for-unsupervised-learning-with-r-and-python/</guid>
      <description>Starting from dimensionality reduction Feature selection is a part technique of data dimensional reduction. According to the book Data minging: concepts and techniques, the most ubiquitous methods are:
 wavelet transforms principal components analysis (PCA) attribute subset selection(or feature selection)  It is worth mentioning, that PCA, Exploratory Factor Analysis (EFA), SVD, etc are all methods which reconstruct our original attributes. PCA is essentially creates new variables that are linear combinations of the original variables.</description>
    </item>
    
    <item>
      <title>Generate your own python package</title>
      <link>uild/2018/05/13/generate-your-own-python-package/</link>
      <pubDate>Sun, 13 May 2018 09:57:55 +0100</pubDate>
      
      <guid>uild/2018/05/13/generate-your-own-python-package/</guid>
      <description>In order to impove the reusability of my</description>
    </item>
    
  </channel>
</rss>